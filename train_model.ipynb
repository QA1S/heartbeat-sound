{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2wKzTU_3Ys_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmpbJe_h67Q6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "42a41655-ac64-4857-89bb-fdc6a99f01ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/combined_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-152189c3355e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcombined_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMBINED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/combined_dataset.csv'"
          ]
        }
      ],
      "source": [
        "# Centralized paths\n",
        "COMBINED_FOLDER = \"./data\"\n",
        "COMBINED_METADATA_PATH = os.path.join(COMBINED_FOLDER, \"combined_dataset.csv\")\n",
        "\n",
        "# Load metadata\n",
        "combined_metadata = pd.read_csv(COMBINED_METADATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca4KvF8ZG0Px"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyL-G0-VrLrA"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None, n_fft=2048, hop_length=512, win_length=None):\n",
        "        \"\"\"\n",
        "        Dataset for audio files with log spectrogram computation.\n",
        "\n",
        "        Parameters:\n",
        "        - file_paths: List of paths to audio files.\n",
        "        - labels: List of labels corresponding to the files.\n",
        "        - transform: Optional torchvision transformations to apply.\n",
        "        - n_fft: Number of FFT components for STFT.\n",
        "        - hop_length: Hop length for STFT.\n",
        "        - win_length: Window length for STFT.\n",
        "        \"\"\"\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.win_length = win_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load audio and compute log spectrogram\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        spectrogram = torchaudio.transforms.Spectrogram(\n",
        "            n_fft=self.n_fft, hop_length=self.hop_length, win_length=self.win_length, power=2.0\n",
        "        )(waveform)\n",
        "        spectrogram = (spectrogram + 1e-6).log2()\n",
        "\n",
        "        # Apply optional transformations\n",
        "        if self.transform:\n",
        "            spectrogram = self.transform(spectrogram)\n",
        "\n",
        "        return spectrogram, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 512)),  # Resize to the input size required by DenseNet\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5]),  # Normalize grayscale\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-YCUe90srsP"
      },
      "outputs": [],
      "source": [
        "combined_metadata[\"label\"] = combined_metadata[\"label\"].astype(\"category\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_paths = combined_metadata[combined_metadata[\"split\"] == \"train\"][\"file_path\"].tolist()\n",
        "train_labels = combined_metadata[combined_metadata[\"split\"] == \"train\"][\"label\"].cat.codes.tolist()\n",
        "\n",
        "val_paths = combined_metadata[combined_metadata[\"split\"] == \"val\"][\"file_path\"].tolist()\n",
        "val_labels = combined_metadata[combined_metadata[\"split\"] == \"val\"][\"label\"].cat.codes.tolist()\n",
        "\n",
        "train_dataset = SpectrogramDataset(train_paths, train_labels, transform=transform)\n",
        "val_dataset = SpectrogramDataset(val_paths, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv4f1hjQNkV5"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBrmDGNII9JH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, num_input_features: int, growth_rate: int, bn_size: int, drop_rate: float) -> None:\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.BatchNorm2d(num_input_features)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "        self.norm2 = nn.BatchNorm2d(bn_size * growth_rate)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, input: List[torch.Tensor]) -> torch.Tensor:\n",
        "        concated_features = torch.cat(input, 1)\n",
        "        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))\n",
        "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return new_features\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.ModuleDict):\n",
        "    def __init__(self, num_layers: int, num_input_features: int, bn_size: int, growth_rate: int, drop_rate: float) -> None:\n",
        "        super().__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(\n",
        "                num_input_features + i * growth_rate,\n",
        "                growth_rate=growth_rate,\n",
        "                bn_size=bn_size,\n",
        "                drop_rate=drop_rate,\n",
        "            )\n",
        "            self.add_module(f\"denselayer{i + 1}\", layer)\n",
        "\n",
        "    def forward(self, init_features: torch.Tensor) -> torch.Tensor:\n",
        "        features = [init_features]\n",
        "        for layer in self.values():\n",
        "            new_features = layer(features)\n",
        "            features.append(new_features)\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features: int, num_output_features: int) -> None:\n",
        "        super().__init__()\n",
        "        self.add_module(\"norm\", nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module(\"relu\", nn.ReLU(inplace=True))\n",
        "        self.add_module(\"conv\", nn.Conv2d(num_input_features, num_output_features, kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module(\"pool\", nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 10,\n",
        "        growth_rate: int = 32,\n",
        "        block_config: Tuple[int, int, int, int] = (6, 12, 32, 32),\n",
        "        num_init_features: int = 64,\n",
        "        bn_size: int = 4,\n",
        "        drop_rate: float = 0.0,\n",
        "        ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution for single-channel input\n",
        "        self.features = nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\"conv0\", nn.Conv2d(1, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "                    (\"norm0\", nn.BatchNorm2d(num_init_features)),\n",
        "                    (\"relu0\", nn.ReLU(inplace=True)),\n",
        "                    (\"pool0\", nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Dense blocks and transitions\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(\n",
        "                num_layers=num_layers,\n",
        "                num_input_features=num_features,\n",
        "                bn_size=bn_size,\n",
        "                growth_rate=growth_rate,\n",
        "                drop_rate=drop_rate,\n",
        "            )\n",
        "            self.features.add_module(f\"denseblock{i + 1}\", block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
        "                self.features.add_module(f\"transition{i + 1}\", trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch normalization\n",
        "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
        "\n",
        "        # Classification layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        features = self.features(x)\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECvsEF38I_bS"
      },
      "outputs": [],
      "source": [
        "classes = [\"normal\", \"artifact\", \"murmur\", \"extrastole\", \"extrahls\"]\n",
        "model = DenseNet(num_classes=len(classes), block_config=(6, 12, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UlYjGnbNiTM"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWW74MrMNdPr"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "scheduler = MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "QsH37BM2NhCE",
        "outputId": "5f0ff51a-d520-4b63-be66-2ab73bcbf705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/90], Loss: 0.8967\n",
            "Validation Loss: 1.0756, Accuracy: 64.10%\n",
            "Epoch [2/90], Loss: 0.8390\n",
            "Validation Loss: 1.0560, Accuracy: 64.10%\n",
            "Epoch [3/90], Loss: 0.8377\n",
            "Validation Loss: 0.9883, Accuracy: 64.10%\n",
            "Epoch [4/90], Loss: 0.8481\n",
            "Validation Loss: 1.0130, Accuracy: 65.81%\n",
            "Epoch [5/90], Loss: 0.8210\n",
            "Validation Loss: 0.9088, Accuracy: 66.67%\n",
            "Epoch [6/90], Loss: 0.8229\n",
            "Validation Loss: 0.9355, Accuracy: 64.96%\n",
            "Epoch [7/90], Loss: 0.8141\n",
            "Validation Loss: 1.1078, Accuracy: 65.81%\n",
            "Epoch [8/90], Loss: 0.8630\n",
            "Validation Loss: 1.0646, Accuracy: 64.96%\n",
            "Epoch [9/90], Loss: 0.8151\n",
            "Validation Loss: 1.6283, Accuracy: 13.68%\n",
            "Epoch [10/90], Loss: 0.8386\n",
            "Validation Loss: 1.3546, Accuracy: 22.22%\n",
            "Epoch [11/90], Loss: 0.8236\n",
            "Validation Loss: 1.9977, Accuracy: 10.26%\n",
            "Epoch [12/90], Loss: 0.7994\n",
            "Validation Loss: 0.9831, Accuracy: 64.96%\n",
            "Epoch [13/90], Loss: 0.7854\n",
            "Validation Loss: 2.7277, Accuracy: 12.82%\n",
            "Epoch [14/90], Loss: 0.7929\n",
            "Validation Loss: 1.7288, Accuracy: 15.38%\n",
            "Epoch [15/90], Loss: 0.7612\n",
            "Validation Loss: 1.3332, Accuracy: 32.48%\n",
            "Epoch [16/90], Loss: 0.7599\n",
            "Validation Loss: 1.4832, Accuracy: 55.56%\n",
            "Epoch [17/90], Loss: 0.7582\n",
            "Validation Loss: 1.2177, Accuracy: 38.46%\n",
            "Epoch [18/90], Loss: 0.7331\n",
            "Validation Loss: 1.1358, Accuracy: 64.96%\n",
            "Epoch [19/90], Loss: 0.7613\n",
            "Validation Loss: 0.9256, Accuracy: 65.81%\n",
            "Epoch [20/90], Loss: 0.7402\n",
            "Validation Loss: 0.8736, Accuracy: 64.96%\n",
            "Epoch [21/90], Loss: 0.7399\n",
            "Validation Loss: 1.2635, Accuracy: 39.32%\n",
            "Epoch [22/90], Loss: 0.7629\n",
            "Validation Loss: 1.1117, Accuracy: 55.56%\n",
            "Epoch [23/90], Loss: 0.7060\n",
            "Validation Loss: 1.7616, Accuracy: 12.82%\n",
            "Epoch [24/90], Loss: 0.7505\n",
            "Validation Loss: 1.0356, Accuracy: 64.96%\n",
            "Epoch [25/90], Loss: 0.7177\n",
            "Validation Loss: 0.9351, Accuracy: 64.96%\n",
            "Epoch [26/90], Loss: 0.7136\n",
            "Validation Loss: 0.8994, Accuracy: 65.81%\n",
            "Epoch [27/90], Loss: 0.6965\n",
            "Validation Loss: 0.9961, Accuracy: 58.12%\n",
            "Epoch [28/90], Loss: 0.6956\n",
            "Validation Loss: 1.4267, Accuracy: 38.46%\n",
            "Epoch [29/90], Loss: 0.7141\n",
            "Validation Loss: 3.7516, Accuracy: 27.35%\n",
            "Epoch [30/90], Loss: 0.6683\n",
            "Validation Loss: 1.1380, Accuracy: 53.85%\n",
            "Epoch [31/90], Loss: 0.6437\n",
            "Validation Loss: 0.8841, Accuracy: 64.96%\n",
            "Epoch [32/90], Loss: 0.6760\n",
            "Validation Loss: 0.8562, Accuracy: 68.38%\n",
            "Epoch [33/90], Loss: 0.6605\n",
            "Validation Loss: 0.8595, Accuracy: 67.52%\n",
            "Epoch [34/90], Loss: 0.6503\n",
            "Validation Loss: 0.8366, Accuracy: 67.52%\n",
            "Epoch [35/90], Loss: 0.6418\n",
            "Validation Loss: 0.8370, Accuracy: 69.23%\n",
            "Epoch [36/90], Loss: 0.6425\n",
            "Validation Loss: 0.8266, Accuracy: 69.23%\n",
            "Epoch [37/90], Loss: 0.6146\n",
            "Validation Loss: 0.8552, Accuracy: 64.96%\n",
            "Epoch [38/90], Loss: 0.6229\n",
            "Validation Loss: 0.8193, Accuracy: 67.52%\n",
            "Epoch [39/90], Loss: 0.6164\n",
            "Validation Loss: 0.8362, Accuracy: 66.67%\n",
            "Epoch [40/90], Loss: 0.6207\n",
            "Validation Loss: 0.8441, Accuracy: 67.52%\n",
            "Epoch [41/90], Loss: 0.6196\n",
            "Validation Loss: 0.8264, Accuracy: 68.38%\n",
            "Epoch [42/90], Loss: 0.6123\n",
            "Validation Loss: 0.8225, Accuracy: 69.23%\n",
            "Epoch [43/90], Loss: 0.5953\n",
            "Validation Loss: 0.8196, Accuracy: 67.52%\n",
            "Epoch [44/90], Loss: 0.6190\n",
            "Validation Loss: 0.8529, Accuracy: 68.38%\n",
            "Epoch [45/90], Loss: 0.5885\n",
            "Validation Loss: 0.8135, Accuracy: 69.23%\n",
            "Epoch [46/90], Loss: 0.5826\n",
            "Validation Loss: 0.8294, Accuracy: 67.52%\n",
            "Epoch [47/90], Loss: 0.6031\n",
            "Validation Loss: 0.8247, Accuracy: 68.38%\n",
            "Epoch [48/90], Loss: 0.5871\n",
            "Validation Loss: 0.8262, Accuracy: 68.38%\n",
            "Epoch [49/90], Loss: 0.5886\n",
            "Validation Loss: 0.8475, Accuracy: 68.38%\n",
            "Epoch [50/90], Loss: 0.5766\n",
            "Validation Loss: 0.8303, Accuracy: 69.23%\n",
            "Epoch [51/90], Loss: 0.5663\n",
            "Validation Loss: 0.7985, Accuracy: 68.38%\n",
            "Epoch [52/90], Loss: 0.5648\n",
            "Validation Loss: 0.8908, Accuracy: 69.23%\n",
            "Epoch [53/90], Loss: 0.5523\n",
            "Validation Loss: 1.1992, Accuracy: 57.26%\n",
            "Epoch [54/90], Loss: 0.5628\n",
            "Validation Loss: 0.8115, Accuracy: 70.09%\n",
            "Epoch [55/90], Loss: 0.5563\n",
            "Validation Loss: 0.7936, Accuracy: 70.09%\n",
            "Epoch [56/90], Loss: 0.5553\n",
            "Validation Loss: 1.0307, Accuracy: 70.09%\n",
            "Epoch [57/90], Loss: 0.5497\n",
            "Validation Loss: 0.7930, Accuracy: 68.38%\n",
            "Epoch [58/90], Loss: 0.5417\n",
            "Validation Loss: 0.7975, Accuracy: 66.67%\n",
            "Epoch [59/90], Loss: 0.5572\n",
            "Validation Loss: 0.9936, Accuracy: 63.25%\n",
            "Epoch [60/90], Loss: 0.5188\n",
            "Validation Loss: 0.8254, Accuracy: 70.94%\n",
            "Epoch [61/90], Loss: 0.5273\n",
            "Validation Loss: 0.8496, Accuracy: 68.38%\n",
            "Epoch [62/90], Loss: 0.5250\n",
            "Validation Loss: 0.8722, Accuracy: 68.38%\n",
            "Epoch [63/90], Loss: 0.5230\n",
            "Validation Loss: 0.8930, Accuracy: 69.23%\n",
            "Epoch [64/90], Loss: 0.4967\n",
            "Validation Loss: 0.8961, Accuracy: 68.38%\n",
            "Epoch [65/90], Loss: 0.4771\n",
            "Validation Loss: 0.8893, Accuracy: 67.52%\n",
            "Epoch [66/90], Loss: 0.4758\n",
            "Validation Loss: 0.8833, Accuracy: 68.38%\n",
            "Epoch [67/90], Loss: 0.4879\n",
            "Validation Loss: 0.8817, Accuracy: 69.23%\n",
            "Epoch [68/90], Loss: 0.4838\n",
            "Validation Loss: 0.8950, Accuracy: 67.52%\n",
            "Epoch [69/90], Loss: 0.4915\n",
            "Validation Loss: 0.8928, Accuracy: 69.23%\n",
            "Epoch [70/90], Loss: 0.4806\n",
            "Validation Loss: 0.8840, Accuracy: 67.52%\n",
            "Epoch [71/90], Loss: 0.4842\n",
            "Validation Loss: 0.8915, Accuracy: 66.67%\n",
            "Epoch [72/90], Loss: 0.4848\n",
            "Validation Loss: 0.8771, Accuracy: 67.52%\n",
            "Epoch [73/90], Loss: 0.4919\n",
            "Validation Loss: 0.8598, Accuracy: 69.23%\n",
            "Epoch [74/90], Loss: 0.4589\n",
            "Validation Loss: 0.8638, Accuracy: 70.09%\n",
            "Epoch [75/90], Loss: 0.4630\n",
            "Validation Loss: 0.8690, Accuracy: 69.23%\n",
            "Epoch [76/90], Loss: 0.4671\n",
            "Validation Loss: 0.8797, Accuracy: 67.52%\n",
            "Epoch [77/90], Loss: 0.4573\n",
            "Validation Loss: 0.8720, Accuracy: 69.23%\n",
            "Epoch [78/90], Loss: 0.4944\n",
            "Validation Loss: 0.8542, Accuracy: 70.09%\n",
            "Epoch [79/90], Loss: 0.4700\n",
            "Validation Loss: 0.8447, Accuracy: 70.09%\n",
            "Epoch [80/90], Loss: 0.4572\n",
            "Validation Loss: 0.8722, Accuracy: 70.09%\n",
            "Epoch [81/90], Loss: 0.4874\n",
            "Validation Loss: 0.9007, Accuracy: 68.38%\n",
            "Epoch [82/90], Loss: 0.4607\n",
            "Validation Loss: 0.9021, Accuracy: 68.38%\n",
            "Epoch [83/90], Loss: 0.4709\n",
            "Validation Loss: 0.8979, Accuracy: 68.38%\n",
            "Epoch [84/90], Loss: 0.4677\n",
            "Validation Loss: 0.9057, Accuracy: 67.52%\n",
            "Epoch [85/90], Loss: 0.4644\n",
            "Validation Loss: 0.9094, Accuracy: 67.52%\n",
            "Epoch [86/90], Loss: 0.4536\n",
            "Validation Loss: 0.8885, Accuracy: 67.52%\n",
            "Epoch [87/90], Loss: 0.4446\n",
            "Validation Loss: 0.8999, Accuracy: 67.52%\n",
            "Epoch [88/90], Loss: 0.4440\n",
            "Validation Loss: 0.8899, Accuracy: 68.38%\n",
            "Epoch [89/90], Loss: 0.4638\n",
            "Validation Loss: 0.9058, Accuracy: 69.23%\n",
            "Epoch [90/90], Loss: 0.4477\n",
            "Validation Loss: 0.8956, Accuracy: 67.52%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 90\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for spectrograms, labels in train_loader:\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(spectrograms)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for spectrograms, labels in val_loader:\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            outputs = model(spectrograms)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# torch.save(model.state_dict(), \"model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "csc413project-o0BZaaHZ",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}